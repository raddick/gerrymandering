{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install xlrd\n",
    "debug = 2\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import geopandas\n",
    "import os\n",
    "from pprint import pprint\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "g = 0\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data for one year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read variable names and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "thisyear = 2018\n",
    "# 2018: Wrote out 73,056 tracts in 36 minutes 2 seconds!\n",
    "# 2017: Wrote out 73,056 tracts in 35 minutes 27 seconds!\n",
    "# 2016: Wrote out 73,056 tracts in 35 minutes 56 seconds!\n",
    "# 2015: Wrote out 73,056 tracts in 33 minutes 34 seconds!\n",
    "# 2014: Wrote out 73,056 tracts in 33 minutes 10 seconds!\n",
    "# 2013: Wrote out 73,056 tracts in 34 minutes 23 seconds!\n",
    "# 2012: Wrote out 73,056 tracts in 36 minutes 32 seconds!\n",
    "# 2011: Wrote out 73,056 tracts in 32 minutes 45 seconds!\n",
    "# 2010: Wrote out 73,057 tracts in 34 minutes 10 seconds!\n",
    "\n",
    "category = 'gerrymandering'\n",
    "\n",
    "basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/acs5/'\n",
    "yeardir = basedir + str(thisyear) + '/'\n",
    "\n",
    "if (thisyear == 2016):\n",
    "    rawdatadir = yeardir + 'rawdata/data/tab4/sumfile/prod/2012thru2016/group2/'\n",
    "elif (thisyear in [2015, 2013, 2011]):\n",
    "    rawdatadir = yeardir + 'rawdata/group2/'\n",
    "elif (thisyear == 2014):\n",
    "    rawdatadir = yeardir + 'rawdata/tab4/sumfile/prod/2010thru2014/group2/'\n",
    "elif (thisyear == 2012):\n",
    "    rawdatadir = yeardir + 'rawdata/tab4/sumfile/prod/2008thru2012/group2/'\n",
    "else:\n",
    "    rawdatadir = yeardir + 'rawdata/'\n",
    "metadir = yeardir + 'metadata/'\n",
    "vardir = yeardir + 'variables/'\n",
    "geodir = yeardir + 'geography/'\n",
    "estimates_dir = yeardir + 'estimates/'\n",
    "margins_of_error_dir = yeardir + 'margins_of_error/'\n",
    "\n",
    "extras_dir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "\n",
    "all_variables_df = pandas.read_csv(vardir+'variables_acs_5yr_all.csv', low_memory=False, index_col='rownumber')\n",
    "\n",
    "# pre-2017, description is blank for FILEID, FILETYPE, STUSAB, CHARITER, SEQUENCE, LOGRECNO - so fill in with blank\n",
    "all_variables_df = all_variables_df.fillna('')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out which variables and sequences we need for our work\n",
    "\n",
    "See the code at the bottom of this file for how I got all the variables we are getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_variables_df[all_variables_df['description'].apply(lambda x: 'vacant' in x.lower())][0:10]\n",
    "\n",
    "# # for i in range(11,50):\n",
    "# #     print('\\'B01001_{0:03d}\\','.format(i))\n",
    "\n",
    "# all_variables_df[all_variables_df['variable'].isin(['B25002_001','B25002_002','B25002_003'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which variables are we getting?\n",
    "varlist = ['FILEID','FILETYPE','STUSAB','CHARITER','SEQUENCE','LOGRECNO','B01001_001']\n",
    "# for_cra_analysis_mac_varlist = ['FILEID',\n",
    "#  'FILETYPE',\n",
    "#  'STUSAB',\n",
    "#  'CHARITER',\n",
    "#  'SEQUENCE',\n",
    "#  'LOGRECNO',\n",
    "# # 'B00001_001',\n",
    "# # 'B00002_001',\n",
    "#  'B01001_001',\n",
    "#  'B02001_001',\n",
    "#  'B02001_002',\n",
    "#  'B02001_003',\n",
    "#  'B08013_001',\n",
    "#  'B11001_001',\n",
    "#  'B11001_002',\n",
    "#  'B11001_006',\n",
    "#  'B11001_007',\n",
    "#  'B11001A_001', \n",
    "#  'B11001B_001',\n",
    "# 'B01001_011',\n",
    "# 'B01001_012',\n",
    "# 'B01001_013',\n",
    "# 'B01001_014',\n",
    "# 'B01001_015',\n",
    "# 'B01001_016',\n",
    "# 'B01001_017',\n",
    "# 'B01001_018',\n",
    "# 'B01001_019',\n",
    "# 'B01001_020',\n",
    "# 'B01001_021',\n",
    "# 'B01001_022',\n",
    "# 'B01001_023',\n",
    "# 'B01001_024',\n",
    "# 'B01001_025',\n",
    "# 'B01001_026',\n",
    "# 'B01001_027',\n",
    "# 'B01001_028',\n",
    "# 'B01001_029',\n",
    "# 'B01001_030',\n",
    "# 'B01001_031',\n",
    "# 'B01001_032',\n",
    "# 'B01001_033',\n",
    "# 'B01001_034',\n",
    "# 'B01001_035',\n",
    "# 'B01001_036',\n",
    "# 'B01001_037',\n",
    "# 'B01001_038',\n",
    "# 'B01001_039',\n",
    "# 'B01001_040',\n",
    "# 'B01001_041',\n",
    "# 'B01001_042',\n",
    "# 'B01001_043',\n",
    "# 'B01001_044',\n",
    "# 'B01001_045',\n",
    "# 'B01001_046',\n",
    "# 'B01001_047',\n",
    "# 'B01001_048',\n",
    "# 'B01001_049',                                                                \n",
    "#  'B15003_001',\n",
    "#  'B15003_002',\n",
    "#  'B15003_003',\n",
    "#  'B15003_004',\n",
    "#  'B15003_005',\n",
    "#  'B15003_006',\n",
    "#  'B15003_007',\n",
    "#  'B15003_008',\n",
    "#  'B15003_009',\n",
    "#  'B15003_010',\n",
    "#  'B15003_011',\n",
    "#  'B15003_012',\n",
    "#  'B15003_013',\n",
    "#  'B15003_014',\n",
    "#  'B15003_015',\n",
    "#  'B15003_016',\n",
    "#  'B15003_017',\n",
    "#  'B15003_018',\n",
    "#  'B15003_019',\n",
    "#  'B15003_020',\n",
    "#  'B15003_021',\n",
    "#  'B15003_022',\n",
    "#  'B15003_023',\n",
    "#  'B15003_024',\n",
    "#  'B15003_025',\n",
    "#  'B17001_001',\n",
    "#  'B17001_002',\n",
    "#  'B17001A_001',\n",
    "#  'B17001A_002',\n",
    "#  'B17001B_001',\n",
    "#  'B17001B_002',\n",
    "#  'B19013_001',\n",
    "#  'B19013A_001',\n",
    "#  'B19013B_001',\n",
    "#  'B19113_001',\n",
    "#  'B23025_002',\n",
    "#  'B23025_005',\n",
    "#  'B25002_001',\n",
    "#  'B25002_002',\n",
    "#  'B25002_003',\n",
    "#  'B25003_001',\n",
    "#  'B25003_002',\n",
    "#  'B25003_003',\n",
    "#  'B25003A_001',\n",
    "#  'B25003A_002',\n",
    "#  'B25003A_003',\n",
    "#  'B25003B_001',\n",
    "#  'B25003B_002',\n",
    "#  'B25003B_003',\n",
    "#  'B25077_001',\n",
    "#  'B25034_001',\n",
    "#  'B25035_001']\n",
    "\n",
    "# all_variables_df[all_variables_df['variable'].isin(for_cra_analysis_mac_varlist)]['description'][5:50]\n",
    "\n",
    "# Which sequences will we need?\n",
    "sequence_list = all_variables_df[all_variables_df['variable'].isin(varlist)]['sequence_number'].drop_duplicates().tolist()\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "sequence_list\n",
    "\n",
    "\n",
    "#for_cra_analysis_mac_varlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing estimates and margins of error for sequence 1...\n",
      "Importing estimates and margins of error for sequence 2...\n",
      "checking whether the dataset has the variables we need...\n",
      "keeping only the variables we need...\n",
      "\n",
      "Found estimates for 290,795 tracts and margins of error for 290,795 tracts in 1 minutes 28 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "state_codes_df = pandas.read_csv(extras_dir+'statecodes.csv')\n",
    "state_codes_df = state_codes_df.set_index('STUSAB')\n",
    "\n",
    "estimates_df = pandas.DataFrame()\n",
    "margins_of_error_df = pandas.DataFrame()\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = [x.lower() for x in states if x not in ('AS', 'GU', 'MP', 'PR', 'UM', 'VI')]\n",
    "\n",
    "for i in sequence_list:\n",
    "    if (debug >= 1):\n",
    "        print('Importing estimates and margins of error for sequence {0:,.0f}...'.format(i))\n",
    "    #print(all_variables_df[all_variables_df['sequence_number'] == i]['variable'].tolist()[0:10])\n",
    "    \n",
    "    this_seq_estimates_df = pandas.DataFrame()\n",
    "    this_seq_margins_of_error_df = pandas.DataFrame()\n",
    "    for onestate in states:\n",
    "#        if (debug >= 2):\n",
    "#            print('\\tImporting estimates and margins of error for {0:}...'.format(onestate))\n",
    "        state_estimates_filename = rawdatadir + 'e{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i)\n",
    "        state_margins_of_error_filename = rawdatadir + 'm{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i)\n",
    "#         if ('e{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i) not in os.listdir(rawdatadir)):\n",
    "#             print('{0:} not found!'.format(state_estimates_filename))\n",
    "\n",
    "        onestate_estiamtes_df = pandas.read_csv(state_estimates_filename, header=None, sep=',', encoding='utf-8', low_memory=False)\n",
    "        this_seq_estimates_df = this_seq_estimates_df.append(onestate_estiamtes_df)\n",
    "        onestate_margins_of_error_df = pandas.read_csv(state_margins_of_error_filename, header=None, sep=',', encoding='utf-8', low_memory=False)\n",
    "        this_seq_margins_of_error_df = this_seq_margins_of_error_df.append(onestate_margins_of_error_df)\n",
    "    if (i >= 2):\n",
    "        #print(this_seq_estimates_df.columns.tolist())\n",
    "        this_seq_estimates_df = this_seq_estimates_df.drop([0,1,2,3,4,5], axis=1)  # ['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO']\n",
    "        this_seq_margins_of_error_df = this_seq_margins_of_error_df.drop([0,1,2,3,4,5], axis=1)  # ['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO']\n",
    "\n",
    "    this_seq_estimates_df.columns = all_variables_df[all_variables_df['sequence_number'] == i]['variable'].tolist()\n",
    "    this_seq_margins_of_error_df.columns = all_variables_df[all_variables_df['sequence_number'] == i]['variable'].tolist()\n",
    "    \n",
    "    estimates_df = pandas.concat((estimates_df,this_seq_estimates_df), axis=1, sort=False)\n",
    "    margins_of_error_df = pandas.concat((margins_of_error_df,this_seq_margins_of_error_df), axis=1, sort=False)\n",
    "    \n",
    "print('checking whether the dataset has the variables we need...')\n",
    "\n",
    "#requested_variables = all_variables_df[all_variables_df['sequence_number'] == 1]['variable'].tolist()\n",
    "requested_variables = varlist\n",
    "\n",
    "variables_to_get = []\n",
    "variables_not_found = []\n",
    "for x in requested_variables:\n",
    "    if x in estimates_df.columns:\n",
    "        variables_to_get.append(x)\n",
    "    else:\n",
    "        variables_not_found.append(x)\n",
    "\n",
    "        \n",
    "# if (len(variables_not_found) > 0):\n",
    "#     print('Variables not found:')\n",
    "#     for y in variables_not_found:\n",
    "#         print('{0:}: {1:}\\n'.format(y, all_variables_df[all_variables_df['variable'] == y]['description'].values[0]))\n",
    "\n",
    "print('keeping only the variables we need...')\n",
    "estimates_df = estimates_df[variables_to_get]\n",
    "margins_of_error_df = margins_of_error_df[variables_to_get]\n",
    "\n",
    "#all_variables_df[all_variables_df['variable'].isin(variables_not_found)]['description']\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('\\nFound estimates for {0:,.0f} tracts and margins of error for {1:,.0f} tracts in {2:,.0f} minutes {3:,.0f} seconds!'.format(len(estimates_df), len(margins_of_error_df), np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "\n",
    "\n",
    "#estimates_df['STUSAB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading geography files...\n",
      "Reading geography for al...\n",
      "Reading geography for ak...\n",
      "Reading geography for az...\n",
      "Reading geography for ar...\n",
      "Reading geography for ca...\n",
      "Reading geography for co...\n",
      "Reading geography for ct...\n",
      "Reading geography for de...\n",
      "Reading geography for dc...\n",
      "Reading geography for fl...\n",
      "Reading geography for ga...\n",
      "Reading geography for hi...\n",
      "Reading geography for id...\n",
      "Reading geography for il...\n",
      "Reading geography for in...\n",
      "Reading geography for ia...\n",
      "Reading geography for ks...\n",
      "Reading geography for ky...\n",
      "Reading geography for la...\n",
      "Reading geography for me...\n",
      "Reading geography for md...\n",
      "Reading geography for ma...\n",
      "Reading geography for mi...\n",
      "Reading geography for mn...\n",
      "Reading geography for ms...\n",
      "Reading geography for mo...\n",
      "Reading geography for mt...\n",
      "Reading geography for ne...\n",
      "Reading geography for nv...\n",
      "Reading geography for nh...\n",
      "Reading geography for nj...\n",
      "Reading geography for nm...\n",
      "Reading geography for ny...\n",
      "Reading geography for nc...\n",
      "Reading geography for nd...\n",
      "Reading geography for oh...\n",
      "Reading geography for ok...\n",
      "Reading geography for or...\n",
      "Reading geography for pa...\n",
      "Reading geography for ri...\n",
      "Reading geography for sc...\n",
      "Reading geography for sd...\n",
      "Reading geography for tn...\n",
      "Reading geography for tx...\n",
      "Reading geography for ut...\n",
      "Reading geography for vt...\n",
      "Reading geography for va...\n",
      "Reading geography for wa...\n",
      "Reading geography for wv...\n",
      "Reading geography for wi...\n",
      "Reading geography for wy...\n",
      "Retaining only tract- and block-group-level geographies...\n",
      "backing up...\n",
      "Done in 2 minutes 4 seconds!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>State</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Logical Record Number</td>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GEOID</td>\n",
       "      <td>14000US01001020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Geography Name</td>\n",
       "      <td>Census Tract 201, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STUSAB</td>\n",
       "      <td>al</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            1767\n",
       "State                  AL                                       \n",
       "Logical Record Number  1768                                     \n",
       "GEOID                  14000US01001020100                       \n",
       "Geography Name         Census Tract 201, Autauga County, Alabama\n",
       "STUSAB                 al                                       "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = time.time()\n",
    "geo_df = pandas.DataFrame()\n",
    "\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = [x.lower() for x in states if x not in ('AS', 'GU', 'MP', 'PR', 'UM', 'VI')]\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Reading geography files...')\n",
    "for onestate in states:\n",
    "    if (thisyear >= 2016):\n",
    "        filename = geodir+'{0:}.xlsx'.format(onestate)\n",
    "    else:\n",
    "        filename = geodir+'{0:}.xls'.format(onestate)\n",
    "    if (debug == 2):\n",
    "        print('Reading geography for {0:}...'.format(onestate))\n",
    "    this_geo_df = pandas.read_excel(filename)\n",
    "    geo_df = pandas.concat((geo_df, this_geo_df), sort=False)\n",
    "    if (thisyear >= 2017):\n",
    "        geo_df = geo_df.assign(STUSAB = geo_df['State'].apply(lambda x: x.lower()))\n",
    "    else:\n",
    "        geo_df = geo_df.assign(STUSAB = geo_df['STATE'].apply(lambda x: x.lower()))\n",
    "\n",
    "geo_df = geo_df.rename(columns={'Geography ID': 'GEOID'})\n",
    "\n",
    "if (debug >= 1):\n",
    "#    print('Retaining only tractlevel geographies...')\n",
    "     print('Retaining only tract- and block-group-level geographies...')\n",
    "#geo_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '140'))]\n",
    "geo_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '140')) | (geo_df['GEOID'].apply(lambda x: x[0:3] == '150'))]\n",
    "\n",
    "print('backing up...')\n",
    "geo_df_bk = geo_df\n",
    "\n",
    "##geo_df = geo_df.set_index('GEOID')  #We'll set GEOID as index colum AFTER the merge\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done in {0:.0f} minutes {1:,.0f} seconds!'.format( np.floor((e-s)/60), (e-s) % 60))\n",
    "\n",
    "#geo_df.sample(1)\n",
    "#print('skipping...')\n",
    "geo_df.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join geography to estimates and margins of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total estimate of B01001_001: 645,806,060\n",
      "Margin of error in B01001_001: 97,634,641\n",
      "\n",
      "\n",
      "Writing data...\n",
      "Estimates written to estimates_acs2018_tract_bg_gerrymandering.csv \n",
      "Margins of error written to margin_of_error_acs2018_tract_gerrymandering.csv\n",
      "\n",
      "Wrote out 290,795 tracts in 3 minutes 45 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# estimates_df = estimates_df_bk\n",
    "# margins_of_error_df = margins_of_error_df_bk\n",
    "# geo_df = geo_df_bk\n",
    "\n",
    "#.sort_values('Logical Record Number')\n",
    "#estimates_df[['STUSAB','LOGRECNO']].dtypes #object, int64\n",
    "#geo_df[['STUSAB', 'Logical Record Number']].dtypes\n",
    "\n",
    "if (thisyear >= 2017):\n",
    "    estimates_df = estimates_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "    margins_of_error_df = margins_of_error_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "# elif (thisyear == 2016):\n",
    "#     estimates_df = estimates_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "#     margins_of_error_df = margins_of_error_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "else:\n",
    "    estimates_df = estimates_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "    margins_of_error_df = margins_of_error_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "\n",
    "estimates_df = estimates_df.set_index('GEOID')  # set the GEOID after we add the shapefiles\n",
    "margins_of_error_df = margins_of_error_df.set_index('GEOID')  # set the GEOID after we add the shapefiles\n",
    "\n",
    "\n",
    "print('Total estimate of B01001_001: {0:,.0f}'.format(estimates_df['B01001_001'].sum()))\n",
    "print('Margin of error in B01001_001: {0:,.0f}'.format(margins_of_error_df['B01001_001'].sum()))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Writing data...')\n",
    "\n",
    "estimates_df.to_csv(estimates_dir + 'estimates_acs{0:.0f}_tract_bg_{1:}.csv'.format(thisyear, category), encoding='utf-8')\n",
    "print('Estimates written to estimates_acs{0:.0f}_tract_bg_{1:}.csv '.format(thisyear, category))\n",
    "      \n",
    "margins_of_error_df.to_csv(margins_of_error_dir + 'margins_of_error_acs{0:.0f}_tract_{1:}.csv'.format(thisyear, category), encoding='utf-8')\n",
    "print('Margins of error written to margin_of_error_acs{0:.0f}_tract_{1:}.csv'.format(thisyear, category))\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('\\nWrote out {0:,.0f} tracts in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(estimates_df), np.floor((g)/60), np.floor((g)%60)))\n",
    "\n",
    "#margins_of_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILEID</th>\n",
       "      <th>FILETYPE</th>\n",
       "      <th>STUSAB</th>\n",
       "      <th>CHARITER</th>\n",
       "      <th>SEQUENCE</th>\n",
       "      <th>LOGRECNO</th>\n",
       "      <th>B01001_001</th>\n",
       "      <th>State</th>\n",
       "      <th>Logical Record Number</th>\n",
       "      <th>Geography Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14000US01001020100</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1768</td>\n",
       "      <td>1923</td>\n",
       "      <td>AL</td>\n",
       "      <td>1768</td>\n",
       "      <td>Census Tract 201, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000US01001020200</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1769</td>\n",
       "      <td>2028</td>\n",
       "      <td>AL</td>\n",
       "      <td>1769</td>\n",
       "      <td>Census Tract 202, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000US01001020300</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1770</td>\n",
       "      <td>3476</td>\n",
       "      <td>AL</td>\n",
       "      <td>1770</td>\n",
       "      <td>Census Tract 203, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000US01001020400</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1771</td>\n",
       "      <td>3831</td>\n",
       "      <td>AL</td>\n",
       "      <td>1771</td>\n",
       "      <td>Census Tract 204, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000US01001020500</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1772</td>\n",
       "      <td>9883</td>\n",
       "      <td>AL</td>\n",
       "      <td>1772</td>\n",
       "      <td>Census Tract 205, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000US560459511001</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>944</td>\n",
       "      <td>1435</td>\n",
       "      <td>WY</td>\n",
       "      <td>944</td>\n",
       "      <td>Block Group 1, Census Tract 9511, Weston County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000US560459511002</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>945</td>\n",
       "      <td>1851</td>\n",
       "      <td>WY</td>\n",
       "      <td>945</td>\n",
       "      <td>Block Group 2, Census Tract 9511, Weston County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000US560459513001</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>946</td>\n",
       "      <td>1225</td>\n",
       "      <td>WY</td>\n",
       "      <td>946</td>\n",
       "      <td>Block Group 1, Census Tract 9513, Weston County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000US560459513002</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>947</td>\n",
       "      <td>1214</td>\n",
       "      <td>WY</td>\n",
       "      <td>947</td>\n",
       "      <td>Block Group 2, Census Tract 9513, Weston County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000US560459513003</td>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201800000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>948</td>\n",
       "      <td>1375</td>\n",
       "      <td>WY</td>\n",
       "      <td>948</td>\n",
       "      <td>Block Group 3, Census Tract 9513, Weston County, Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290795 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    FILEID     FILETYPE STUSAB  CHARITER  SEQUENCE  LOGRECNO  \\\n",
       "GEOID                                                                          \n",
       "14000US01001020100   ACSSF  201800000.0  al     0         1         1768       \n",
       "14000US01001020200   ACSSF  201800000.0  al     0         1         1769       \n",
       "14000US01001020300   ACSSF  201800000.0  al     0         1         1770       \n",
       "14000US01001020400   ACSSF  201800000.0  al     0         1         1771       \n",
       "14000US01001020500   ACSSF  201800000.0  al     0         1         1772       \n",
       "...                    ...          ...  ..    ..        ..          ...       \n",
       "15000US560459511001  ACSSF  201800000.0  wy     0         1         944        \n",
       "15000US560459511002  ACSSF  201800000.0  wy     0         1         945        \n",
       "15000US560459513001  ACSSF  201800000.0  wy     0         1         946        \n",
       "15000US560459513002  ACSSF  201800000.0  wy     0         1         947        \n",
       "15000US560459513003  ACSSF  201800000.0  wy     0         1         948        \n",
       "\n",
       "                     B01001_001 State  Logical Record Number  \\\n",
       "GEOID                                                          \n",
       "14000US01001020100   1923        AL    1768                    \n",
       "14000US01001020200   2028        AL    1769                    \n",
       "14000US01001020300   3476        AL    1770                    \n",
       "14000US01001020400   3831        AL    1771                    \n",
       "14000US01001020500   9883        AL    1772                    \n",
       "...                   ...        ..     ...                    \n",
       "15000US560459511001  1435        WY    944                     \n",
       "15000US560459511002  1851        WY    945                     \n",
       "15000US560459513001  1225        WY    946                     \n",
       "15000US560459513002  1214        WY    947                     \n",
       "15000US560459513003  1375        WY    948                     \n",
       "\n",
       "                                                               Geography Name  \n",
       "GEOID                                                                          \n",
       "14000US01001020100   Census Tract 201, Autauga County, Alabama                 \n",
       "14000US01001020200   Census Tract 202, Autauga County, Alabama                 \n",
       "14000US01001020300   Census Tract 203, Autauga County, Alabama                 \n",
       "14000US01001020400   Census Tract 204, Autauga County, Alabama                 \n",
       "14000US01001020500   Census Tract 205, Autauga County, Alabama                 \n",
       "...                                                        ...                 \n",
       "15000US560459511001  Block Group 1, Census Tract 9511, Weston County, Wyoming  \n",
       "15000US560459511002  Block Group 2, Census Tract 9511, Weston County, Wyoming  \n",
       "15000US560459513001  Block Group 1, Census Tract 9513, Weston County, Wyoming  \n",
       "15000US560459513002  Block Group 2, Census Tract 9513, Weston County, Wyoming  \n",
       "15000US560459513003  Block Group 3, Census Tract 9513, Weston County, Wyoming  \n",
       "\n",
       "[290795 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = pandas.read_csv(estimates_dir+'estimates_acs{0:}_tract_bg_gerrymandering.csv'.format(thisyear), encoding='utf-8', low_memory=False, index_col='GEOID')\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
