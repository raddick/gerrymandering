{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "this_state = 'NH'\n",
    "this_state_name = 'New Hampshire'\n",
    "the_crs_epsg = 3438    #  https://spatialreference.org/ref/epsg/nad83-new-hampshire-ftus/\n",
    "\n",
    "from ftplib import FTP\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import pandas\n",
    "import numpy as np\n",
    "import time\n",
    "import geopandas\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "g = 0\n",
    "\n",
    "show_water = True\n",
    "show_roads = False\n",
    "water_area_tol = 1 * 1000 * 1000\n",
    "\n",
    "map_buffer_ratio = 0.05\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "script_dir = '/home/idies/workspace/Storage/raddick/jordanraddick.com/gerrymandering/'\n",
    "census_script_dir = '/home/idies/workspace/Storage/raddick/census/'\n",
    "basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/redistricting/2020/'\n",
    "shapefile_basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/shapefiles/2020/'\n",
    "extras_dir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "\n",
    "official_new_districts_dir = '/home/idies/workspace/Temporary/raddick/census_scratch/redistricting/actual_new_districts/'\n",
    "\n",
    "\n",
    "sandbox_dir = '/home/idies/workspace/Storage/raddick/jordanraddick.com/gerrymandering/sandbox/'\n",
    "temp_sandbox_dir = '/home/idies/workspace/Temporary/raddick/census_scratch/redistricting/sandbox/'\n",
    "\n",
    "\n",
    "district_color_cycle = ['blue', 'red', 'green', 'orange', 'cyan', 'yellow', 'pink', 'gray', 'lime', 'navajowhite', 'cornflowerblue', 'darkseagreen', 'thistle', 'tomato', 'silver', 'blueviolet', 'olive', 'peru', 'dodgerblue']\n",
    "# district_color_cycle += district_color_cycle\n",
    "# district_color_cycle += district_color_cycle\n",
    "\n",
    "map_buffer_ratio = .1\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find districts and target population for this state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from FTP...\n",
      "Got 14 apportionment Excel files in 2.1 seconds!\n",
      "saving apportionment table...\n",
      "Processed and saved apportionment data in 0.2 seconds!\n",
      "\n",
      "\n",
      "New Hampshire has 1,377,529 residents and 1,560 overseas...\n",
      "\n",
      "\n",
      "New Hampshire now has 2 House districts, each with a target population of 689,545!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('Getting data from FTP...')\n",
    "\n",
    "ftp = FTP('ftp2.census.gov')\n",
    "ftp.login()\n",
    "ftp.cwd('programs-surveys/decennial/2020/data/apportionment/')\n",
    "for thisfile in [x for x in ftp.nlst() if x[-5:] == '.xlsx']:\n",
    "    with io.open(sandbox_dir+'apportionment/'+thisfile, 'wb') as f:\n",
    "        ftp.retrbinary('RETR {0:}'.format(thisfile), f.write)\n",
    "ftp.quit()\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('Got {0:.0f} apportionment Excel files in {1:.1f} seconds!'.format(len(os.listdir(sandbox_dir+'apportionment')), e-s))\n",
    "\n",
    "s = time.time()\n",
    "table1_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-table01.xlsx', header=3)\n",
    "table1_df = table1_df.rename(columns={'STATE': 'state_name'})\n",
    "for thiscol in [x for x in table1_df.columns if '\\n' in x]:\n",
    "    table1_df = table1_df.rename(columns = {thiscol: thiscol.replace('\\n', '')})\n",
    "table1_df = table1_df.rename(columns = {'NUMBER OF APPORTIONED REPRESENTATIVES BASED ON 2020 CENSUS2': 'NUMBER OF APPORTIONED REPRESENTATIVES BASED ON 2020 CENSUS'})\n",
    "table1_df = table1_df.head(len(table1_df)-2)\n",
    "table1_df = table1_df.set_index('state_name')\n",
    "\n",
    "table2_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-table02.xlsx', header=3)\n",
    "table2_df = table2_df.head(len(table2_df)-4)\n",
    "table2_df = table2_df.drop('This cell is intentionally blank.', axis=1)\n",
    "table2_df = table2_df.rename(columns={'AREA': 'state_name'})\n",
    "table2_df = table2_df.set_index('state_name')\n",
    "\n",
    "table3_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-table03.xlsx', header=3)\n",
    "table3_df = table3_df.head(len(table3_df)-2)\n",
    "table3_df = table3_df.drop('This cell is intentionally blank.', axis=1)\n",
    "table3_df = table3_df.rename(columns={'STATE': 'state_name'})\n",
    "table3_df = table3_df.set_index('state_name')\n",
    "\n",
    "#tablea_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-tableA.xlsx')\n",
    "#tableb_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-tableB.xlsx')    # states considered for gaining and losing seats\n",
    "#tablec1_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-tableC1.xlsx')   # historic number of seats from 1910 to 2020\n",
    "#tablec2_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-tableC2.xlsx')   # historic population and seat changes from 1910 to 2020\n",
    "#tabled_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-tableD.xlsx')    # seats gained and lost in 2020 by state\n",
    "#tablee_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-2020-tableE.xlsx')    # percent changes in population and rank of state thereby\n",
    "#popchange_df = pandas.read_excel(sandbox_dir+'apportionment/population-change-data-table.xlsx')     # change in population by state and census region from decade to decade (1910-2020)\n",
    "#popdensity_df = pandas.read_excel(sandbox_dir+'apportionment/population-density-data-table.xlsx')     # historic population density from 1910 to 2020\n",
    "\n",
    "apportionment_df = pandas.read_excel(sandbox_dir+'apportionment/apportionment-data-table.xlsx', header=4)\n",
    "apportionment_df = apportionment_df.rename(columns={'Unnamed: 0': 'state_name'})\n",
    "apportionment_df = apportionment_df.drop([x for x in apportionment_df.columns if 'Unnamed' in x], axis=1)\n",
    "\n",
    "for thiscol in [x for x in apportionment_df.columns if '\\n' in x]:\n",
    "    apportionment_df = apportionment_df.rename(columns = {thiscol: thiscol.replace('\\n', ' (')+')'})\n",
    "apportionment_df = apportionment_df.head(len(apportionment_df)-4)\n",
    "\n",
    "apportionment_df = apportionment_df.set_index('state_name')\n",
    "\n",
    "apportionment_df = apportionment_df.join(table1_df['APPORTIONMENT POPULATION (APRIL 1, 2020)'])\n",
    "apportionment_df = apportionment_df[['APPORTIONMENT POPULATION (APRIL 1, 2020)'] + [x for x in apportionment_df if x != 'APPORTIONMENT POPULATION (APRIL 1, 2020)']]\n",
    "\n",
    "apportionment_df = apportionment_df.join(table2_df['RESIDENT POPULATION (APRIL 1, 2020)'])\n",
    "\n",
    "apportionment_df = apportionment_df.join(table3_df['OVERSEAS POPULATION (APRIL 1, 2020)'])\n",
    "\n",
    "print('saving apportionment table...')\n",
    "apportionment_df.to_csv(temp_sandbox_dir+'apportionment.csv')\n",
    "\n",
    "nDistricts = int(apportionment_df.loc[this_state_name][[x for x in apportionment_df.columns if '2020' in x]]['Number of Seats 2020 Census'])\n",
    "target_pop = int(apportionment_df.loc[this_state_name][[x for x in apportionment_df.columns if '2020' in x]]['Average Population Per Seat (2020 Census)'])\n",
    "\n",
    "# apportionment_df.loc[:, 'nDistricts'] = pandas.to_numeric(apportionment_df['nDistricts'], downcast='integer', errors='coerce')\n",
    "# apportionment_df.loc[:, 'target_pop'] = pandas.to_numeric(apportionment_df['target_pop'], downcast='integer', errors='coerce')\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('Processed and saved apportionment data in {0:.1f} seconds!'.format(e-s))\n",
    "print('\\n')\n",
    "print('{0:} has {1:,.0f} residents and {2:,.0f} overseas...'.format(this_state_name, apportionment_df.loc[this_state_name]['RESIDENT POPULATION (APRIL 1, 2020)'], apportionment_df.loc[this_state_name]['OVERSEAS POPULATION (APRIL 1, 2020)']))\n",
    "print('\\n')\n",
    "print('{0:} now has {1:.0f} House districts, each with a target population of {2:,.0f}!'.format(this_state_name, nDistricts, target_pop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get redistricting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from FTP...\n",
      "220-    **WARNING**WARNING**WARNING**WARNING**WARNING****WARNING**WARNING**\n",
      "220-\n",
      "220-    This is a Census Bureau computer system. Census Bureau computer \n",
      "220-    systems are provided for the processing of official U.S. Government \n",
      "220-    information only. All data contained within Census Bureau computer \n",
      "220-    systems is owned by the Census Bureau, and may be monitored,intercepted, recorded, read, \n",
      "220-    copied, or captured in any manner and disclosed in any manner, by \n",
      "220-    authorized personnel. THERE IS NO RIGHT OF PRIVACY IN THIS SYSTEM. \n",
      "220-    System personnel may disclose any potential evidence of crime found \n",
      "220-    on Census Bureau computer systems to appropriate authorities. USE OF \n",
      "220-    THIS SYSTEM BY ANY USER, AUTHORIZED OR UNAUTHORIZED, CONSTITUTES \n",
      "220-    CONSENT TO THIS MONITORING, INTERCEPTION RECORDING, READING, COPYING, \n",
      "220-    CAPTURING, and DISCLOSURE OF COMPUTER ACTIVITY. Use of this computer \n",
      "220-    without authorization or for unauthorized purposes is a violation of \n",
      "220-    federal law and punishable by fines or imprisonment (Public Law 99-474).\n",
      "220-\n",
      "220-    **WARNING**WARNING**WARNING**WARNING**WARNING****WARNING**WARNING**\n",
      "220 \n",
      "['nh2020.pl.zip']\n",
      "getting data...\n",
      "unzipping...\n",
      "/home/idies/workspace/Temporary/raddick/census_scratch/redistricting/sandbox/nh2020.pl.zip\n",
      "Extracting nh2020.pl.zip...\n",
      "\n",
      "\n",
      "Retrieved and unzipped files from Census FTP in 11.8 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('Getting data from FTP...')\n",
    "\n",
    "ftp = FTP('ftp2.census.gov')\n",
    "ftp.login()\n",
    "print(ftp.getwelcome())\n",
    "\n",
    "# ftp.cwd('programs-surveys/decennial/2020/data/01-Redistricting_File--PL_94-171/{0:}'.format(this_state_name))\n",
    "ftp.cwd('programs-surveys/decennial/2020/data/01-Redistricting_File--PL_94-171/{0:}'.format(this_state_name.replace(\" \",\"_\")))\n",
    "print(ftp.nlst())\n",
    "filename = '{0:}/{1:}'.format(temp_sandbox_dir, ftp.nlst()[0])\n",
    "print('getting data...')\n",
    "with io.open(filename, 'wb') as f:\n",
    "    ftp.retrbinary('RETR {0:}'.format(ftp.nlst()[0]), f.write)\n",
    "ftp.quit()\n",
    "\n",
    "print('unzipping...')\n",
    "os.chdir(temp_sandbox_dir)\n",
    "for thisfile in [x for x in os.listdir(temp_sandbox_dir) if x[-4:] == '.zip']:\n",
    "    print(temp_sandbox_dir+thisfile)\n",
    "    with zipfile.ZipFile(temp_sandbox_dir+thisfile, 'r') as z:\n",
    "        print('Extracting {0:}...'.format(thisfile))\n",
    "        z.extractall()\n",
    "for thisfile in [x for x in os.listdir() if ('zip' in x)]:\n",
    "    os.remove(thisfile)\n",
    "os.chdir(script_dir)\n",
    "\n",
    "#os.getcwd()\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "#print('Done in {0:.1f} seconds!'.format(e-s))\n",
    "print('\\n')\n",
    "print('Retrieved and unzipped files from Census FTP in {0:,.1f} seconds!'.format(e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get summary levels (so we can tell what units of analysis we are dealing with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up summary levels...\n",
      "Read descriptions for 85 summary levels in 0.0 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('setting up summary levels...')\n",
    "sumlevel_df = pandas.read_excel(script_dir+'sumlevel.xlsx')\n",
    "sumlevel_df = sumlevel_df.rename(columns={'sumlevel': 'SUMLEV'})\n",
    "sumlevel_df.index.name = 'rownumber'\n",
    "#sumlevel_df = sumlevel_df.set_index('SUMLEV')\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('Read descriptions for {0:,.0f} summary levels in {1:.1f} seconds!'.format(len(sumlevel_df), e-s))\n",
    "#sumlevel_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read geo data (all levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading redistricting geography data for New Hampshire...\n",
      "renaming columns...\n",
      "\tTotal columns: 97\n",
      "\tNamed columns: 97\n",
      "\n",
      "\n",
      "Read 44,463 geographies in 0 minutes 2 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('reading redistricting geography data for {0:}...'.format(this_state_name))\n",
    "\n",
    "geofile = '{0:}{1:}geo2020.pl'.format(temp_sandbox_dir, this_state.lower())\n",
    "geo_df = pandas.read_csv(geofile, sep='|', header=None, low_memory=False, encoding='ISO-8859-1')#, encoding='utf-8')\n",
    "\n",
    "print('renaming columns...')\n",
    "column_names = []\n",
    "column_names += ['FILEID', 'STUSAB', 'SUMLEV', 'GEOVAR', 'GEOCOMP', 'CHARITER', 'CIFSN', 'LOGRECNO', 'GEOID']\n",
    "column_names += ['GEOCODE', 'REGION', 'DIVISION', 'STATE', 'STATENS', 'COUNTY', 'COUNTYCC', 'COUNTYNS']\n",
    "column_names += ['COUSUB', 'COUSUBCC', 'COUSUBNS']\n",
    "column_names += ['SUBMCD', 'SUBMDCC', 'SUBMCDNS', 'ESTATE', 'ESTATECC', 'ESTATENS']\n",
    "column_names += ['CONCIT', 'CONCITCC', 'CONCITNS', 'PLACE','PLACECC', 'PLACENS']\n",
    "column_names += ['TRACT', 'BLKGRP', 'BLOCK']\n",
    "column_names += ['AIANHH', 'AIANHHLI', 'AIANHHFP', 'AIANHHCC', 'AIANHHNS', 'AITS', 'AITSFP', 'AITSCC', 'AITSNS']\n",
    "column_names += ['TTRACT', 'TBLKGRP', 'ANRC', 'ANRCCC', 'ARNCNS']\n",
    "column_names += ['CBSA', 'MEMI', 'CSA', 'METDIV']\n",
    "column_names += ['NECTA', 'NMEMI', 'CNECTA', 'NECTADIV']\n",
    "column_names += ['CBSAPCI', 'NECTAPCI', 'UA', 'UATYPE', 'UR']\n",
    "column_names += ['CD116', 'CD118', 'CD119', 'CD120', 'CD121']\n",
    "column_names += ['SLDU18', 'SLDU22', 'SLDU24', 'SLDU26', 'SLDU28']\n",
    "column_names += ['SLDL18', 'SLDL22', 'SLDL24', 'SLDL26', 'SLDL28']\n",
    "column_names += ['VTD', 'VTDI', 'ZCTA', 'SDELM', 'SDSEC', 'SDUNI', 'PUMA']\n",
    "column_names += ['AREALAND', 'AREAWATER', 'BASENAME', 'NAME', 'FUNCSTAT', 'GCUNI']\n",
    "column_names += ['POP100', 'HU100']\n",
    "column_names += ['INTPTLAT', 'INTPTLON', 'LSADC', 'PARTFLAG', 'UGA']\n",
    "geo_df.columns = column_names\n",
    "\n",
    "print('\\tTotal columns: {0:.0f}'.format(len(geo_df.columns)))\n",
    "print('\\tNamed columns: {0:.0f}'.format(len(column_names)))\n",
    "\n",
    "geo_df = geo_df.set_index('GEOID')\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('\\n')\n",
    "print('Read {0:,.0f} geographies in {1:.0f} minutes {2:.0f} seconds!'.format(len(geo_df), np.floor((e-s)/60), (e-s)%60))\n",
    "#geo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count units of analysis by summary level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting by summary level...\n",
      "\n",
      "\n",
      "Documented 85 geographies in 0.1 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('counting by summary level...')\n",
    "count_df = pandas.DataFrame(data=None, columns=['sumlev_description', 'count', 'pop'], index=sumlevel_df['SUMLEV'].tolist())\n",
    "count_df.index.name = 'SUMLEV'\n",
    "count_df.loc[:, 'sumlev_description'] = sumlevel_df.set_index('SUMLEV')['description']\n",
    "count_df.loc[:, 'count'] = geo_df.groupby('SUMLEV').size()\n",
    "count_df.loc[:, 'pop'] = geo_df.groupby('SUMLEV')['POP100'].sum()\n",
    "#count_df\n",
    "nan_levels = count_df[count_df['count'].isnull()].index.tolist()\n",
    "\n",
    "htmlstr = ''\n",
    "htmlstr += '<table>'\n",
    "htmlstr += '<tr>'\n",
    "htmlstr += '<th>SUMLEVEL</th>'\n",
    "for thiscol in count_df.columns.tolist():    \n",
    "    htmlstr += '<th>{0:}</th>'.format(thiscol)\n",
    "htmlstr += '</tr>'\n",
    "\n",
    "for sumlev, thisrow in count_df.iterrows():\n",
    "    #if ('remainder' not in thisrow['sumlev_description'].lower()):\n",
    "    if (sumlev not in nan_levels):\n",
    "        htmlstr += '<tr>'    \n",
    "        htmlstr += '<td>{0:3d}</td><td>{1:}</td><td>{2:,.0f}</td><td>{3:,.0f}</td>'.format(sumlev, thisrow['sumlev_description'], thisrow['count'], thisrow['pop'])\n",
    "        htmlstr += '</tr>'\n",
    "htmlstr += '</table>'\n",
    "\n",
    "# print('No data for these columns: {0:}'.format(nan_levels))\n",
    "# display(HTML(htmlstr))\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('\\n')\n",
    "print('Documented {0:,.0f} geographies in {1:.1f} seconds!'.format(len(count_df), e-s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 2010 Congressonal District data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current Congressional District data...\n",
      "adding shapefiles...\n",
      "\n",
      "\n",
      "Joined 2 congressional districts to shapefiles in 3.0 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('Getting current Congressional District data...')\n",
    "cd_df = geo_df[geo_df['SUMLEV'] == 500]\n",
    "cd_df = cd_df[cd_df['CD116'] != 'ZZ']\n",
    "cd_df.loc[:, 'CD116'] = pandas.to_numeric(cd_df['CD116'], errors='coerce')\n",
    "cd_df.loc[cd_df['CD116'] == 0, 'CD116'] = 1\n",
    "\n",
    "print('adding shapefiles...')\n",
    "gdf = geopandas.read_file(shapefile_basedir+'CD/tl_2020_us_cd116.shp')\n",
    "gdf.loc[:, 'GEOID'] = gdf['GEOID'].apply(lambda x: '5001600US'+x)\n",
    "gdf = gdf.set_index('GEOID')\n",
    "\n",
    "cd_gdf = geopandas.GeoDataFrame(data=cd_df.join(gdf.geometry), crs=gdf.crs, geometry='geometry')\n",
    "#cd_gdf\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('\\n')\n",
    "print('Joined {0:,.0f} congressional districts to shapefiles in {1:.1f} seconds!'.format(len(cd_gdf), e-s))\n",
    "\n",
    "#cd_gdf.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get census tract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tract data...\n",
      "looking up state names from numbers...\n",
      "looking up county names from numbers...\n",
      "adding empty column for new districts...\n",
      "adding shapefiles...\n",
      "\tgetting shapefiles for NH...\n",
      "\n",
      "\n",
      "Joined 350 tracts to shapefiles in 10.5 seconds!\n"
     ]
    }
   ],
   "source": [
    "print('getting tract data...')\n",
    "s = time.time()\n",
    "tract_df = geo_df[geo_df['SUMLEV'] == 140]\n",
    "tract_df = tract_df.assign(census_tract = tract_df['TRACT'].apply(lambda x: x/100))\n",
    "#tract_df[['SUMLEV', 'STATE', 'STUSAB', 'COUNTY', 'census_tract', 'BLKGRP', 'NAME']].sample(3)\n",
    "\n",
    "print('looking up state names from numbers...')\n",
    "state_codes_df = pandas.read_csv(extras_dir+'statecodes.csv')\n",
    "tract_df = tract_df.reset_index().merge(state_codes_df[['STATE', 'STATE_NAME']], how='left', on='STATE').set_index('GEOID')\n",
    "\n",
    "\n",
    "print('looking up county names from numbers...')\n",
    "county_names_df = pandas.read_excel(extras_dir+'all-geocodes-v2019.xlsx', header=4)\n",
    "county_names_df = county_names_df[county_names_df['Summary Level'] == 50]\n",
    "county_names_df = county_names_df.rename(columns={'State Code (FIPS)': 'STATE', 'County Code (FIPS)': 'COUNTY', 'Area Name (including legal/statistical area description)': 'COUNTY_NAME' })\n",
    "county_names_df = county_names_df.reset_index(drop=True)\n",
    "tract_df = tract_df.reset_index().merge(county_names_df[['STATE', 'COUNTY', 'COUNTY_NAME']], how='left', on=['STATE', 'COUNTY']).set_index('GEOID')\n",
    "\n",
    "print('adding empty column for new districts...')\n",
    "tract_df = tract_df.assign(new_district = np.nan)\n",
    "\n",
    "print('adding shapefiles...')\n",
    "gdf = geopandas.GeoDataFrame()\n",
    "for this_state_key in tract_df['STATE'].drop_duplicates().tolist():\n",
    "    print('\\tgetting shapefiles for {0:}...'.format(state_codes_df[state_codes_df['STATE'] == this_state_key]['STUSAB'].values[0]))\n",
    "    gdf_i = geopandas.read_file(shapefile_basedir+'TRACT/tl_2020_{0:02d}_tract.shp'.format(this_state_key))\n",
    "    #print('\\t\\t{0:} CRS = {1:}'.format(this_state_key, gdf_i.crs))\n",
    "    gdf = pandas.concat((gdf, gdf_i), axis=0)\n",
    "gdf.loc[:, 'GEOID'] = gdf['GEOID'].apply(lambda x: '1400000US'+x)\n",
    "gdf = gdf.set_index('GEOID')\n",
    "tract_gdf = geopandas.GeoDataFrame(data=tract_df.join(gdf.geometry), crs=gdf.crs, geometry='geometry')\n",
    "\n",
    "\n",
    "#state_codes_df\n",
    "#tract_df = tract_gdf.reset_index().merge(state_codes_df[['STATE', 'STATE_NAME']], how='left', on='STATE').set_index('GEOID')\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('\\n')\n",
    "print('Joined {0:,.0f} tracts to shapefiles in {1:.1f} seconds!'.format(len(tract_gdf), e-s))\n",
    "\n",
    "#tract_gdf.plot()                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ancillary data for making plots\n",
    "\n",
    "State, counties, water areas, metro areas, places, maybe roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting state...\n",
      "getting counties...\n",
      "getting water areas...\n",
      "\treading water file 0 of 10...\n",
      "Got 121 water areas in 0 minutes 8.2 seconds!\n",
      "\n",
      "\n",
      "getting CBSAs (metro areas)...\n",
      "\tClassifying CBSAs by type and spatial extent...\n",
      "\tFinding populations of CBSAs...\n",
      "Got 7 CBSAs in 2.3 seconds!\n",
      "\n",
      "\n",
      "getting places...\n",
      "\treading place file 0 of 1...\n",
      "Got 100 places in 0.1 seconds!\n",
      "\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "state_numbers_include_list = cd_gdf['STATE'].drop_duplicates().tolist()\n",
    "state_numbers_include_list = ['{0:02d}'.format(x) for x in state_numbers_include_list]\n",
    "#state_numbers_include_list\n",
    "\n",
    "print('getting state...')\n",
    "state_gdf = geopandas.read_file(shapefile_basedir+'tl_2020_us_state.shp')\n",
    "state_gdf.loc[:, 'INTPTLON'] = pandas.to_numeric(state_gdf['INTPTLON'])\n",
    "state_gdf.loc[:, 'INTPTLAT'] = pandas.to_numeric(state_gdf['INTPTLAT'])\n",
    "state_gdf.loc[:, 'STATEFP'] = pandas.to_numeric(state_gdf['STATEFP'], errors='coerce')\n",
    "state_gdf = state_gdf[state_gdf['STATEFP'].isin(state_numbers_include_list)]\n",
    "state_gdf = state_gdf.set_index('GEOID')\n",
    "\n",
    "print('getting counties...')\n",
    "county_gdf = geopandas.read_file(shapefile_basedir+'tl_2020_us_county.shp')\n",
    "county_gdf.loc[:, 'STATEFP'] = pandas.to_numeric(county_gdf['STATEFP'], errors='coerce')\n",
    "county_gdf.loc[:, 'COUNTYNS'] = pandas.to_numeric(county_gdf['COUNTYNS'], errors='coerce')\n",
    "county_gdf.loc[:, 'INTPTLON'] = pandas.to_numeric(county_gdf['INTPTLON'])\n",
    "county_gdf.loc[:, 'INTPTLAT'] = pandas.to_numeric(county_gdf['INTPTLAT'])\n",
    "county_gdf = county_gdf[county_gdf['STATEFP'].isin(state_numbers_include_list)]\n",
    "county_gdf = county_gdf.set_index('GEOID')\n",
    "\n",
    "\n",
    "if (show_water):\n",
    "    print('getting water areas...')\n",
    "\n",
    "    waterfiles = [shapefile_basedir+'AREAWATER/'+x for x in os.listdir(shapefile_basedir+'AREAWATER/') if ((x[-4:] == '.shp') )]\n",
    "    state_numbers_include_list = cd_gdf['STATE'].drop_duplicates().tolist()\n",
    "    state_numbers_include_list = ['{0:02d}'.format(x) for x in state_numbers_include_list]\n",
    "\n",
    "    waterfiles = [x for x in waterfiles if x[89:91] in state_numbers_include_list]\n",
    "\n",
    "    water_gdf = geopandas.GeoDataFrame()\n",
    "\n",
    "    for i in range(0, len(waterfiles)):\n",
    "        if (np.mod(i, 10) == 0):\n",
    "            print('\\treading water file {0:,.0f} of {1:,.0f}...'.format(i, len(waterfiles)))\n",
    "        water_gdf_i = geopandas.read_file(waterfiles[i])\n",
    "        water_gdf_i = water_gdf_i[water_gdf_i['AWATER'] >= water_area_tol]\n",
    "        water_gdf_i = water_gdf_i.assign(STUSAB = state_codes_df[state_codes_df['STATE'] == int(waterfiles[i][89:91])]['STUSAB'].values[0])\n",
    "        water_gdf = pandas.concat((water_gdf, water_gdf_i), axis=0)\n",
    "    water_gdf.loc[:, 'INTPTLON'] = pandas.to_numeric(water_gdf['INTPTLON'], errors='coerce')\n",
    "    water_gdf.loc[:, 'INTPTLAT'] = pandas.to_numeric(water_gdf['INTPTLAT'], errors='coerce')\n",
    "    water_gdf = water_gdf.set_index('HYDROID')\n",
    "    \n",
    "    e = time.time()\n",
    "    g += (e-s)\n",
    "\n",
    "    print('Got {0:,.0f} water areas in {1:.0f} minutes {2:.1f} seconds!'.format(len(water_gdf), np.floor((e-s)/60), (e-s)%60))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "print('getting CBSAs (metro areas)...')\n",
    "cbsa_gdf = geopandas.read_file(shapefile_basedir+'CBSA/tl_2020_us_cbsa.shp')\n",
    "cbsa_gdf = cbsa_gdf[cbsa_gdf['NAME'].apply(lambda x: this_state in x[x.find(',')+2:])]\n",
    "cbsa_gdf.loc[:, 'INTPTLON'] = pandas.to_numeric(cbsa_gdf['INTPTLON'])\n",
    "cbsa_gdf.loc[:, 'INTPTLAT'] = pandas.to_numeric(cbsa_gdf['INTPTLAT'])\n",
    "cbsa_gdf.loc[:, 'MEMI'] = pandas.to_numeric(cbsa_gdf['MEMI'])\n",
    "cbsa_gdf = cbsa_gdf.set_index('GEOID')\n",
    "\n",
    "print('\\tClassifying CBSAs by type and spatial extent...')\n",
    "cbsa_gdf = cbsa_gdf.assign(cbsa_type = np.nan)\n",
    "cbsa_gdf.loc[cbsa_gdf['MEMI'] == 1, 'cbsa_type'] = 'metro'\n",
    "cbsa_gdf.loc[cbsa_gdf['MEMI'] == 2, 'cbsa_type'] = 'micro'\n",
    "\n",
    "cbsa_gdf = cbsa_gdf.assign(extends_beyond = False)\n",
    "cbsa_gdf = cbsa_gdf.assign(extends_into = False)\n",
    "\n",
    "cbsa_gdf.loc[\n",
    "    (cbsa_gdf['NAME'].apply(lambda x: (len(x[x.find(',')+2:]) > 2)))\n",
    "    & (cbsa_gdf['NAME'].apply(lambda x: x[x.find(',')+2:x.find(',')+4] == this_state)), \n",
    "    'extends_beyond'] = True\n",
    "\n",
    "\n",
    "cbsa_gdf.loc[\n",
    "    (cbsa_gdf['NAME'].apply(lambda x: (len(x[x.find(',')+2:]) > 2)))\n",
    "    & (cbsa_gdf['NAME'].apply(lambda x: x[x.find(',')+2:x.find(',')+4] != this_state)), \n",
    "    'extends_into'] = True\n",
    "\n",
    "print('\\tFinding populations of CBSAs...')\n",
    "cbsa_gdf = cbsa_gdf.assign(POP100 = np.nan)\n",
    "for ix, thisrow in cbsa_gdf.iterrows():\n",
    "    cbsa_gdf.loc[ix, 'POP100'] = tract_gdf[(tract_gdf['COUNTYNS'].isin(county_gdf[county_gdf.geometry.within(thisrow.geometry)]['COUNTYNS'].tolist()))]['POP100'].sum()\n",
    "#     if (this_state in thisrow['NAME']):\n",
    "#         print('\\t\\t{0:} (n = {1:,.0f})...'.format(thisrow['NAME'], cbsa_gdf.loc[ix]['POP100']))\n",
    "cbsa_gdf.loc[cbsa_gdf[cbsa_gdf['POP100'] == 0].index.tolist(), 'POP100'] = np.nan\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('Got {0:,.0f} CBSAs in {1:.1f} seconds!'.format(len(cbsa_gdf), e-s))\n",
    "print('\\n')\n",
    "s = time.time()\n",
    "print('getting places...')\n",
    "placefiles = [shapefile_basedir+'PLACE/'+x for x in os.listdir(shapefile_basedir+'PLACE/') if ((x[-4:] == '.shp'))]\n",
    "placefiles = [x for x in placefiles if x[85:87] in state_numbers_include_list]\n",
    "\n",
    "place_gdf = geopandas.GeoDataFrame()\n",
    "\n",
    "for i in range(0, len(placefiles)):\n",
    "    if (np.mod(i, 5) == 0):\n",
    "        print('\\treading place file {0:,.0f} of {1:,.0f}...'.format(i, len(placefiles)))\n",
    "    place_gdf_i = geopandas.read_file(placefiles[i])\n",
    "    place_gdf = pandas.concat((place_gdf, place_gdf_i), axis=0)\n",
    "place_gdf.loc[:, 'STATEFP'] = pandas.to_numeric(place_gdf['STATEFP'])\n",
    "place_gdf.loc[:, 'INTPTLON'] = pandas.to_numeric(place_gdf['INTPTLON'])\n",
    "place_gdf.loc[:, 'INTPTLAT'] = pandas.to_numeric(place_gdf['INTPTLAT'])\n",
    "\n",
    "place_gdf.loc[:, 'GEOID'] = place_gdf['GEOID'].apply(lambda x: '1600000US'+x)\n",
    "place_gdf = place_gdf.set_index('GEOID')\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "print('Got {0:,.0f} places in {1:.1f} seconds!'.format(len(place_gdf), e-s))\n",
    "print('\\n')\n",
    "\n",
    "if (show_roads):\n",
    "    s = time.time()\n",
    "    print('getting roads...')\n",
    "    roads_gdf = geopandas.GeoDataFrame()\n",
    "    roads_file_list = [shapefile_basedir+'ROADS/'+x for x in os.listdir(shapefile_basedir+'ROADS/') if ((x[-4:] == '.shp') and (x[8:10] in state_numbers_include_list))]# and ('_{0:02d}'.format()))]# and ('tl_2018_{0:02d}'.format(this_state_number) in x))]\n",
    "    for i in range(0, len(roads_file_list)):\n",
    "        if ((np.mod(i,10) == 0) | (i == len(roads_file_list)-1)):\n",
    "            print('\\tReading road file {0:,.0f} of {1:,.0f}...'.format(i+1, len(roads_file_list)))\n",
    "        roads_gdf_i = geopandas.read_file(roads_file_list[i])    \n",
    "        roads_gdf = pandas.concat((roads_gdf, roads_gdf_i), axis=0, sort=False)\n",
    "    roads_gdf = roads_gdf.set_index('LINEARID')\n",
    "    e = time.time()\n",
    "    g += (e-s)\n",
    "    print('Got {0:,.0f} roads in {1:.0f} minutes {2:.1f} seconds!'.format(len(roads_gdf), np.floor((e-s)/60), (e-s)%60))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "#cbsa_gdf[['NAME', 'MEMI', 'cbsa_type', 'extends_beyond', 'extends_into', 'POP100']].sort_values(by='POP100', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign each tract to its new district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting from backup...\n",
      "Assigning districts by county...\n",
      "Assigning districts by place...\n",
      "\n",
      "\n",
      "Assigning districts by tract number...\n",
      "All the rest are district 2...\n",
      "Assigned districts to 350 tracts in 0 minutes 0 seconds!\n",
      "\n",
      "\n",
      "District 1: n = 689,011 (100.0% of target; overshot = 246)\n",
      "District 2: n = 688,518 (100.0% of target; overshot = -247)\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "#this_state = 'RI'\n",
    "\n",
    "# nDistricts = these_states_dict[this_state]['seats_new']\n",
    "# state_target = these_states_dict[this_state]['target']\n",
    "\n",
    "print('getting from backup...')\n",
    "tract_gdf.loc[tract_gdf['STUSAB'] == this_state, 'new_district'] = np.nan\n",
    "\n",
    "county_mapper_df = pandas.DataFrame(data=[    \n",
    "    ['Hillsborough County',1],\n",
    "], columns=['COUNTY_NAME', 'new_district_by_county'])\n",
    "county_mapper_df = county_mapper_df.assign(STUSAB = this_state)\n",
    "\n",
    "place_mapper_df = pandas.DataFrame(data=[\n",
    "    ['Londonderry',1],\n",
    "    ['Derry',1],\n",
    "], columns=['NAME', 'new_district_by_place'])\n",
    "\n",
    "\n",
    "print('Assigning districts by county...')\n",
    "tract_gdf.loc[tract_gdf[tract_gdf['STUSAB'] == this_state].index, 'new_district'] = tract_gdf[tract_gdf['STUSAB'] == this_state].reset_index().merge(county_mapper_df, how='left', on=['STUSAB','COUNTY_NAME']).set_index('GEOID')['new_district_by_county']\n",
    "\n",
    "\n",
    "# #print('Assigning districts by lat/lon...')\n",
    "# # tract_gdf.loc[(tract_gdf['STUSAB'] == this_state) & (tract_gdf.index.map(lambda x: x[11:14] == '003')) & (tract_gdf['new_district'].isnull()) & (tract_gdf['INTPTLAT'] >= 21.44) , 'new_district'] = 2   # county 003 is Honolulu County (Oahu)\n",
    "# # tract_gdf.loc[(tract_gdf['STUSAB'] == this_state) & (tract_gdf.index.map(lambda x: x[11:14] == '003')) & (tract_gdf['new_district'].isnull()) & (tract_gdf['INTPTLAT'] < 21.44) , 'new_district'] = np.nan  # county 003 is Honolulu County (Oahu)\n",
    "\n",
    "\n",
    "print('Assigning districts by place...')\n",
    "place_mapper_gdf = geopandas.GeoDataFrame(place_mapper_df.merge(place_gdf.reset_index(), how='left', on='NAME')[['GEOID', 'NAME', 'new_district_by_place', 'geometry']].set_index('GEOID'))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "for i in range(1, nDistricts+1):\n",
    "    tracts_here = []\n",
    "    tracts_within = tract_gdf[(tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'].isnull()) & (tract_gdf.geometry.within(unary_union(place_mapper_gdf[place_mapper_gdf['new_district_by_place'] == i].geometry.tolist())))].index.tolist()\n",
    "    tracts_here += tracts_within\n",
    "    tracts_intersecting = tract_gdf[(tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'].isnull()) \n",
    "              & (tract_gdf.geometry.intersects(unary_union(place_mapper_gdf.geometry.tolist())))\n",
    "              & ~(tract_gdf.index.isin(tracts_within))\n",
    "             ].index.tolist()\n",
    "    for this_tract in tracts_intersecting:\n",
    "        total_tract_area_geo = tract_gdf.loc[this_tract].geometry.area\n",
    "        #total_tract_area_crs = tract_gdf.to_crs(equal_area_crs).loc[this_tract].geometry.area\n",
    "\n",
    "        intersection_tract_area_geo = tract_gdf.loc[this_tract].geometry.intersection(unary_union(place_mapper_gdf[place_mapper_gdf['new_district_by_place'] == i].geometry.tolist())).area\n",
    "        #intersection_tract_area_crs = tract_gdf.to_crs(equal_area_crs).loc[this_tract].geometry.intersection(unary_union(place_mapper_gdf.to_crs(equal_area_crs).geometry.tolist())).area\n",
    "        \n",
    "        intersection_tract_pct = intersection_tract_area_geo / total_tract_area_geo\n",
    "        if (intersection_tract_pct >= 0.5):\n",
    "            tracts_here.append(this_tract)\n",
    "    tract_gdf.loc[tracts_here, 'new_district'] = i\n",
    "\n",
    "print('Assigning districts by tract number...')\n",
    "tract_gdf.loc[['1400000US33015003301', '1400000US33015003302', '1400000US33015003601', '1400000US33015003602', '1400000US33015003703', '1400000US33015003801', '1400000US33015003802', '1400000US33015004000'], 'new_district'] = 1   #   N of Derry\n",
    "tract_gdf.loc[['1400000US33015100100', '1400000US33015100200', '1400000US33015100301', '1400000US33015100302', '1400000US33015100401', '1400000US33015100402', '1400000US33015105100', '1400000US33015106101', '1400000US33015106102'], 'new_district'] = 1    # E of Londonderry\n",
    "tract_gdf.loc[['1400000US33015050000', '1400000US33015051000', '1400000US33015052000', '1400000US33015053000', '1400000US33015054000', '1400000US33015055001', '1400000US33015055002', '1400000US33015059000', '1400000US33015060000'], 'new_district'] = 1   #   S of Hooksett\n",
    "tract_gdf.loc[['1400000US33015102100', '1400000US33015103100', '1400000US33015104101', '1400000US33015104102', '1400000US33015101101', '1400000US33015101102'], 'new_district'] = 1   #   E of Derry\n",
    "tract_gdf.loc[['1400000US33015062000', '1400000US33015062500', '1400000US33015063001', '1400000US33015063003', '1400000US33015063004'], 'new_district'] = 1  # Southern border\n",
    "tract_gdf.loc[['1400000US33015064000', '1400000US33015065005', '1400000US33015065006', '1400000US33015065007', '1400000US33015065008', '1400000US33015065009', '1400000US33015065010'], 'new_district'] = 1  # Southern border\n",
    "tract_gdf.loc[['1400000US33015061001', '1400000US33015106200', '1400000US33015106400'], 'new_district'] = 1  # Exeter\n",
    "tract_gdf.loc[['1400000US33015066000', '1400000US33015067000'], 'new_district'] = 1  # Near Portsmouth\n",
    "tract_gdf.loc[['1400000US33015067502', '1400000US33015067503', '1400000US33015067504', '1400000US33015067505'], 'new_district'] = 1  # Newmarket\n",
    "tract_gdf.loc[['1400000US33015980011'], 'new_district'] = 1  # Misc whatever\n",
    "\n",
    "\n",
    "\n",
    "print('All the rest are district 2...')\n",
    "tract_gdf.loc[(tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'].isnull()), 'new_district'] = 2\n",
    "\n",
    "\n",
    "overseas_pop = apportionment_df.loc[this_state_name]['OVERSEAS POPULATION (APRIL 1, 2020)']\n",
    "overseas_each_district = overseas_pop/2\n",
    "\n",
    "overseas_adj = {}\n",
    "overseas_adj[1] = np.ceil(overseas_each_district)\n",
    "overseas_adj[2] = np.floor(overseas_each_district)\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "\n",
    "print('Assigned districts to {0:,.0f} tracts in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(tract_gdf['new_district'].dropna()), np.floor((e-s)/60), (e-s)%60))\n",
    "print('\\n')\n",
    "\n",
    "#print(target_pop)\n",
    "for i in range(1, nDistricts+1): \n",
    "    if (tract_gdf[(tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'] == i)]['POP100'].sum() > 0):\n",
    "        print('District {0:}: n = {1:,.0f} ({2:.1%} of target; overshot = {3:,.0f})'.format(\n",
    "            i, \n",
    "            tract_gdf[(tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'] == i)]['POP100'].sum(), \n",
    "            (tract_gdf[(tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'] == i)]['POP100'].sum() + overseas_adj[i])/target_pop, \n",
    "            (tract_gdf[(tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'] == i)]['POP100'].sum() + overseas_adj[i]) - target_pop))\n",
    "\n",
    "        \n",
    "\n",
    "#place_gdf[place_gdf['STATEFP'] == 16].sort_values(by='NAME')[20:]\n",
    "#county_gdf[(county_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])]['NAME'].sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET PLANS FOR NEW DISTRICTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "## https://gerrymander.princeton.edu/reforms/NH\n",
    "## Republican plan: https://gerrymander.princeton.edu/redistricting-report-card/?planId=reccq6JFN3eeiZiox\n",
    "## Democratic plan: https://gerrymander.princeton.edu/redistricting-report-card/?planId=recM42Wa2qgtES3Df\n",
    "### downloaded files: NH-C21Rep.geojson and NH-C21Dem.geojson\n",
    "### geojson converted to shp using https://mygeodata.cloud/converter/geojson-to-shp\n",
    "\n",
    "# official_cd118_gdf = geopandas.read_file('{0:}/{1:}/RI_CD_Enacted02162022.shp'.format(official_new_districts_dir, this_state_name.replace(' ','_').lower()))\n",
    "# official_cd118_gdf.loc[:, 'DISTRICT'] = pandas.to_numeric(official_cd118_gdf['DISTRICT'], downcast='integer', errors='coerce')\n",
    "# official_cd118_gdf = official_cd118_gdf.set_index('ID')\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "\n",
    "print('Got {0:.0f} officially sanctioned new districts for {1:} in {2:,.1f} seconds!'.format(len(official_cd118_gdf), this_state_name, e-s))\n",
    "# print(official_cd118_gdf.crs)\n",
    "# print(place_gdf.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "show_places_mainmap = True\n",
    "show_inset_places_mainmap = True\n",
    "label_places = False\n",
    "show_water = True\n",
    "\n",
    "print('setting up insets...')\n",
    "insets = {}\n",
    "insets['providence'] = {}\n",
    "insets['providence']['map_bounds'] = [place_gdf[(place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0]) & (place_gdf['NAME'] == 'Providence')].to_crs(epsg=the_crs_epsg).bounds['minx'].values[0], place_gdf[(place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0]) & (place_gdf['NAME'] == 'Providence')].to_crs(epsg=the_crs_epsg).bounds['miny'].values[0], place_gdf[(place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0]) & (place_gdf['NAME'] == 'Providence')].to_crs(epsg=the_crs_epsg).bounds['maxx'].values[0], place_gdf[(place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0]) & (place_gdf['NAME'] == 'Providence')].to_crs(epsg=the_crs_epsg).bounds['maxy'].values[0]]\n",
    "#insets['providence']['map_bounds'] = [330000,250000,370000,290000]\n",
    "insets['providence']['axes_position'] = [0.69, 0.69]\n",
    "insets['providence']['axes_rel_size'] = .3\n",
    "#insets['providence']['cityname'] = 'Providence'\n",
    "insets['providence']['titlesize'] = 28\n",
    "insets['providence']['borderwidth'] = 6\n",
    "\n",
    "# insets['twinfalls'] = {}\n",
    "# insets['twinfalls']['map_bounds'] = [2.43e6, 1.245e6, 2.5e6, 1.28e6]\n",
    "# insets['twinfalls']['axes_position'] = [0.65, 0.65]\n",
    "# insets['twinfalls']['axes_rel_size'] = .3\n",
    "# insets['twinfalls']['cityname'] = 'Twin Falls'\n",
    "# insets['twinfalls']['titlesize'] = 22\n",
    "# insets['twinfalls']['borderwidth'] = 3\n",
    "\n",
    "inset_cities_list = []\n",
    "for inset_name, this_inset in insets.items():\n",
    "    this_inset['aspect_ratio'] = (this_inset['map_bounds'][2] - this_inset['map_bounds'][0]) / (this_inset['map_bounds'][3] - this_inset['map_bounds'][1])\n",
    "    this_inset['axes_size'] = [this_inset['axes_rel_size'], this_inset['axes_rel_size'] / this_inset['aspect_ratio']]\n",
    "    inset_cities_list.append(inset_name.title())\n",
    "\n",
    "print('reprojecting...')\n",
    "xlimits = [state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['minx'].values[0], state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['maxx'].values[0]]\n",
    "ylimits = [state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['miny'].values[0], state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['maxy'].values[0]]\n",
    "\n",
    "xspan = xlimits[1] - xlimits[0]\n",
    "yspan = ylimits[1] - ylimits[0]\n",
    "aspect_ratio = xspan / yspan\n",
    "\n",
    "xlimits = [xlimits[0] - (xspan * map_buffer_ratio), xlimits[1] + (xspan * map_buffer_ratio)]\n",
    "ylimits = [ylimits[0] - (yspan * map_buffer_ratio), ylimits[1] + (yspan * map_buffer_ratio)]\n",
    "\n",
    "viewport_gdf = geopandas.GeoDataFrame(data=[[box(xlimits[0], ylimits[0], xlimits[1], ylimits[1])]], columns=['geometry'], crs=the_crs_epsg, geometry='geometry')\n",
    "\n",
    "print('plotting...')\n",
    "fig, ax = plt.subplots(1,1,figsize=(24, 24/aspect_ratio))\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "new_district_list = tract_gdf[(tract_gdf['STUSAB'] == this_state)]['new_district'].drop_duplicates().sort_values().tolist()\n",
    "\n",
    "legend_list = []\n",
    "for i in range(1, len(new_district_list)+1):\n",
    "    print('\\tDistrict {0:.0f}...'.format(i))\n",
    "    tract_gdf[\n",
    "        (tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'] == i)\n",
    "    ].to_crs(epsg=the_crs_epsg).plot(ax=ax, color=district_color_cycle[i], edgecolor='black')\n",
    "    legend_list.append(mpatches.Patch(color=district_color_cycle[i], label='Fixed District {0:,.0f}'.format(i)))\n",
    "\n",
    "    \n",
    "if (show_water):\n",
    "    water_gdf.to_crs(the_crs_epsg).plot(ax=ax, color='blue')\n",
    "    \n",
    "if (show_places_mainmap):\n",
    "    print('plotting places...')\n",
    "    geopandas.overlay(\n",
    "        place_gdf[\n",
    "            (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])\n",
    "          ].reset_index().to_crs(the_crs_epsg),\n",
    "        viewport_gdf, \n",
    "        how='intersection').set_index('GEOID').plot(ax=ax, color='none', edgecolor='yellow', lw=1, linestyle='dashed')\n",
    "\n",
    "    if (label_places):\n",
    "        print('labeling places...')\n",
    "        for ix, thisrow in geopandas.overlay(\n",
    "            place_gdf[\n",
    "                (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])\n",
    "              ].reset_index().to_crs(epsg=the_crs_epsg),\n",
    "            viewport_gdf, \n",
    "            how='intersection').set_index('GEOID').iterrows():\n",
    "            annotator = thisrow['NAME']\n",
    "            labelpoint = (thisrow.geometry.centroid.x, thisrow.geometry.centroid.y)\n",
    "            ax.annotate(annotator, labelpoint, \n",
    "                        color='black', backgroundcolor='white', ha='center', va='center', fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "official_cd118_gdf.to_crs(epsg=the_crs_epsg).plot(ax=ax, color='none', edgecolor='white', lw=4)\n",
    "\n",
    "if (show_inset_places_mainmap):\n",
    "    print('plotting places that will appear in insets...')\n",
    "    geopandas.overlay(\n",
    "        place_gdf[\n",
    "            (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])\n",
    "            & (place_gdf['NAME'].isin(inset_cities_list))\n",
    "          ].reset_index().to_crs(epsg=the_crs_epsg),\n",
    "        viewport_gdf, \n",
    "        how='intersection').set_index('GEOID').plot(ax=ax, color='none', edgecolor='yellow', lw=3)\n",
    "\n",
    "if (show_places_mainmap):\n",
    "    print('plotting places...')\n",
    "    geopandas.overlay(\n",
    "        place_gdf[\n",
    "            (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])\n",
    "          ].reset_index().to_crs(the_crs_epsg),\n",
    "        viewport_gdf, \n",
    "        how='intersection').set_index('GEOID').plot(ax=ax, color='none', edgecolor='yellow', lw=1)\n",
    "\n",
    "    if (label_places):\n",
    "        print('labeling places...')\n",
    "        for ix, thisrow in geopandas.overlay(\n",
    "            place_gdf[\n",
    "                (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])\n",
    "              ].reset_index().to_crs(epsg=the_crs_epsg),\n",
    "            viewport_gdf, \n",
    "            how='intersection').set_index('GEOID').iterrows():\n",
    "            annotator = thisrow['NAME']\n",
    "            labelpoint = (thisrow.geometry.centroid.x, thisrow.geometry.centroid.y)\n",
    "            ax.annotate(annotator, labelpoint, \n",
    "                        color='black', backgroundcolor='white', ha='center', va='center', fontsize=20)\n",
    "\n",
    "if (show_inset_places_mainmap):\n",
    "    print('plotting places that will appear in insets...')\n",
    "    geopandas.overlay(\n",
    "        place_gdf[\n",
    "            (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])\n",
    "            & (place_gdf['NAME'].isin(inset_cities_list))\n",
    "          ].reset_index().to_crs(epsg=the_crs_epsg),\n",
    "        viewport_gdf, \n",
    "        how='intersection').set_index('GEOID').plot(ax=ax, color='none', edgecolor='yellow', linestyle='dashed', lw=3)\n",
    "\n",
    "\n",
    "\n",
    "for ix, thisrow in geopandas.overlay(\n",
    "    official_cd118_gdf.reset_index().to_crs(epsg=the_crs_epsg),\n",
    "    viewport_gdf, \n",
    "    how='intersection'\n",
    ").set_index('ID').iterrows():\n",
    "    annotator = 'Official\\nDistrict {0:.0f}'.format(thisrow['DISTRICT'])\n",
    "    if (thisrow['DISTRICT'] == 1):    \n",
    "        labelpoint = (330000,310000)\n",
    "    elif (thisrow['DISTRICT'] == 2):\n",
    "        labelpoint = (thisrow.geometry.centroid.x, 210000)\n",
    "    else:\n",
    "        labelpoint = (thisrow.geometry.centroid.x, thisrow.geometry.centroid.y)\n",
    "    ax.annotate(annotator, labelpoint, \n",
    "                color='black', backgroundcolor='white', ha='center', va='center', fontsize=24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "ax.set_xlim(xlimits)\n",
    "ax.set_ylim(ylimits)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "for this_inset_name, this_inset in insets.items():\n",
    "    print('Plotting inset for {0:}...'.format(this_inset_name))\n",
    "    \n",
    "    xlimits = [this_inset['map_bounds'][0], this_inset['map_bounds'][2]]\n",
    "    ylimits = [this_inset['map_bounds'][1], this_inset['map_bounds'][3]]\n",
    "    \n",
    "    xspan = xlimits[1] - xlimits[0]\n",
    "    yspan = ylimits[1] - ylimits[0]\n",
    "    aspect_ratio = xspan / yspan\n",
    "\n",
    "    xlimits = [xlimits[0] - (xspan * map_buffer_ratio), xlimits[1] + (xspan * map_buffer_ratio)]\n",
    "    ylimits = [ylimits[0] - (yspan * map_buffer_ratio), ylimits[1] + (yspan * map_buffer_ratio)]\n",
    "\n",
    "    #viewport_gdf = geopandas.GeoDataFrame(data=[[box(this_inset['map_bounds'][0], this_inset['map_bounds'][1], this_inset['map_bounds'][2], this_inset['map_bounds'][3])]], columns=['geometry'], crs=the_crs_epsg, geometry='geometry')\n",
    "    viewport_gdf = geopandas.GeoDataFrame(data=[[box(xlimits[0], ylimits[0], xlimits[1], ylimits[1])]], columns=['geometry'], crs=the_crs_epsg, geometry='geometry')\n",
    "    \n",
    "    this_inset['axes'] = ax.inset_axes([this_inset['axes_position'][0], this_inset['axes_position'][1], this_inset['axes_size'][0], this_inset['axes_size'][1]])\n",
    "    \n",
    "    this_inset['axes'].set_aspect('equal')\n",
    "    \n",
    "    for i in range(1, len(new_district_list)+1):\n",
    "        print('\\tDistrict {0:.0f}...'.format(i))\n",
    "        tract_gdf[\n",
    "            (tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'] == i)\n",
    "        ].to_crs(epsg=the_crs_epsg).plot(ax=this_inset['axes'], color=district_color_cycle[i], edgecolor='black')\n",
    "    \n",
    "    water_gdf.to_crs(the_crs_epsg).plot(ax=this_inset['axes'], color='blue')\n",
    "    \n",
    "    official_cd118_gdf.to_crs(the_crs_epsg).plot(ax=this_inset['axes'], color='none', edgecolor='white', lw=6)    \n",
    "    place_gdf[(place_gdf['NAME'] == this_inset_name.title()) & (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])].to_crs(the_crs_epsg).plot(ax=this_inset['axes'], color='none', edgecolor='yellow', lw=4, linestyle='dashed')\n",
    "\n",
    "        \n",
    "    this_inset['axes'].set_xlim(xlimits)\n",
    "    this_inset['axes'].set_ylim(ylimits)\n",
    "    \n",
    "    geopandas.GeoDataFrame(data=[[box(xlimits[0], ylimits[0], xlimits[1], ylimits[1])]], columns=['geometry'], crs=the_crs_epsg, geometry='geometry').plot(ax=this_inset['axes'], color='none', edgecolor='black', lw=insets['providence']['borderwidth'])\n",
    "    this_inset['axes'].set_title(this_inset_name.title(), fontsize=this_inset['titlesize'])\n",
    "    \n",
    "    \n",
    "    \n",
    "#    print('\\n')\n",
    "    this_inset['axes'].set_xticks([])\n",
    "    this_inset['axes'].set_yticks([])\n",
    "\n",
    "ax.legend(handles=legend_list, fontsize=24, loc='upper left')\n",
    "\n",
    "rectpatch, connects = ax.indicate_inset_zoom(this_inset['axes'], edgecolor=\"black\", linewidth=this_inset['borderwidth'], alpha=1)\n",
    "for j in range(0,4):\n",
    "    connects[j].set_visible(False)\n",
    "\n",
    "plt.title('{0:}'.format(this_state_name), fontsize=48)\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig(script_dir+'{0:}.svg'.format(this_state_name.lower().replace(' ','_')), format='svg', bbox_inches='tight')\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "\n",
    "print('Plotted and saved in {0:,.1f} seconds!'.format(e-s))\n",
    "print('\\n')\n",
    "print('Total time: {0:,.1f} seconds!'.format(g))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "print('reprojecting...')\n",
    "xlimits = [state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['minx'].values[0], state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['maxx'].values[0]]\n",
    "ylimits = [state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['miny'].values[0], state_gdf[(state_gdf['STUSPS'] == this_state)].to_crs(epsg=the_crs_epsg).bounds['maxy'].values[0]]\n",
    "\n",
    "xspan = xlimits[1] - xlimits[0]\n",
    "yspan = ylimits[1] - ylimits[0]\n",
    "aspect_ratio = xspan / yspan\n",
    "\n",
    "xlimits = [xlimits[0] - (xspan * map_buffer_ratio), xlimits[1] + (xspan * map_buffer_ratio)]\n",
    "ylimits = [ylimits[0] - (yspan * map_buffer_ratio), ylimits[1] + (yspan * map_buffer_ratio)]\n",
    "\n",
    "viewport_gdf = geopandas.GeoDataFrame(data=[[box(xlimits[0], ylimits[0], xlimits[1], ylimits[1])]], columns=['geometry'], crs=the_crs_epsg, geometry='geometry')\n",
    "\n",
    "print('plotting...')\n",
    "fig, ax = plt.subplots(1,1,figsize=(24, 24/aspect_ratio))\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "new_district_list = tract_gdf[(tract_gdf['STUSAB'] == this_state)]['new_district'].drop_duplicates().sort_values().tolist()\n",
    "\n",
    "legend_list = []\n",
    "for i in range(1, len(new_district_list)+1):\n",
    "    print('\\tDistrict {0:.0f}...'.format(i))\n",
    "    tract_gdf[\n",
    "        (tract_gdf['STUSAB'] == this_state) & (tract_gdf['new_district'] == i)\n",
    "    ].to_crs(epsg=the_crs_epsg).plot(ax=ax, color=district_color_cycle[i], edgecolor='black')\n",
    "    legend_list.append(mpatches.Patch(color=district_color_cycle[i], label='Fixed District {0:,.0f}'.format(i)))\n",
    "\n",
    "water_gdf.to_crs(the_crs_epsg).plot(ax=ax, color='blue')\n",
    "official_cd118_gdf.to_crs(epsg=the_crs_epsg).plot(ax=ax, color='none', edgecolor='white', lw=4)\n",
    "\n",
    "\n",
    "print('plotting places...')\n",
    "print()\n",
    "place_gdf[(place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])].to_crs(the_crs_epsg).plot(ax=ax, color='none', edgecolor='yellow', lw=1)\n",
    "\n",
    "place_gdf[(place_gdf['NAME'].isin(place_mapper_df['NAME'].tolist())) & (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])].to_crs(the_crs_epsg).plot(ax=ax, color='none', edgecolor='yellow', lw=3)\n",
    "\n",
    "\n",
    "for ix, thisrow in geopandas.overlay(\n",
    "    place_gdf[(place_gdf['NAME'].isin(place_mapper_df['NAME'].tolist())) & (place_gdf['STATEFP'] == state_codes_df[state_codes_df['STUSAB'] == this_state]['STATE'].values[0])].to_crs(the_crs_epsg),\n",
    "    viewport_gdf,\n",
    "    how='intersection'\n",
    ").iterrows():\n",
    "    annotator = thisrow['NAME']\n",
    "    labelpoint = (thisrow.geometry.centroid.x, thisrow.geometry.centroid.y)\n",
    "    ax.annotate(annotator, labelpoint, \n",
    "                color='black', backgroundcolor='white', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# for ix, thisrow in geopandas.overlay(\n",
    "#     official_cd118_gdf.reset_index().to_crs(epsg=the_crs_epsg),\n",
    "#     viewport_gdf, \n",
    "#     how='intersection'\n",
    "# ).set_index('ID').iterrows():\n",
    "#     annotator = 'Official\\nDistrict {0:.0f}'.format(thisrow['DISTRICT'])\n",
    "#     if (thisrow['DISTRICT'] == 1):    \n",
    "#         labelpoint = (330000,310000)\n",
    "#     elif (thisrow['DISTRICT'] == 2):\n",
    "#         labelpoint = (thisrow.geometry.centroid.x, 210000)\n",
    "#     else:\n",
    "#         labelpoint = (thisrow.geometry.centroid.x, thisrow.geometry.centroid.y)\n",
    "#     ax.annotate(annotator, labelpoint, \n",
    "#                 color='black', backgroundcolor='white', ha='center', va='center', fontsize=24)\n",
    "\n",
    "ax.set_xlim(xlimits)\n",
    "ax.set_ylim(ylimits)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "\n",
    "ax.legend(handles=legend_list, fontsize=24, loc='upper left')\n",
    "\n",
    "plt.title('{0:}'.format(this_state_name), fontsize=48)\n",
    "\n",
    "#plt.show()\n",
    "fig.savefig(script_dir+'{0:}_cities.svg'.format(this_state_name.lower().replace(' ','_')), format='svg', bbox_inches='tight')\n",
    "\n",
    "e = time.time()\n",
    "g += (e-s)\n",
    "\n",
    "print('Plotted and saved in {0:,.1f} seconds!'.format(e-s))\n",
    "print('\\n')\n",
    "print('Total time: {0:,.1f} seconds!'.format(g))\n",
    "\n",
    "print(target_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('setting up infrastructure to plot roads...')\n",
    "# # we may derive from matplotlib.patches.BoxStyle._Base class.\n",
    "# # You need to override transmute method in this case.\n",
    "# class shield(BoxStyle._Base):\n",
    "#     \"\"\"\n",
    "#     A simple box.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, pad=0.3):\n",
    "#         \"\"\"\n",
    "#         The arguments need to be floating numbers and need to have\n",
    "#         default values.\n",
    "\n",
    "#          *pad*\n",
    "#             amount of padding\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.pad = pad\n",
    "#         super().__init__()\n",
    "\n",
    "#     def transmute(self, x0, y0, width, height, mutation_size):\n",
    "#         \"\"\"\n",
    "#         Given the location and size of the box, return the path of\n",
    "#         the box around it.\n",
    "\n",
    "#          - *x0*, *y0*, *width*, *height* : location and size of the box\n",
    "#          - *mutation_size* : a reference scale for the mutation.\n",
    "\n",
    "#         Often, the *mutation_size* is the font size of the text.\n",
    "#         You don't need to worry about the rotation as it is\n",
    "#         automatically taken care of.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # padding\n",
    "#         pad = mutation_size * self.pad\n",
    "\n",
    "#         # width and height with padding added.\n",
    "#         width, height = width + 2.*pad, \\\n",
    "#                         height + 2.*pad,\n",
    "\n",
    "#         # boundary of the padded box\n",
    "#         x0, y0 = x0-pad, y0-pad,\n",
    "#         x1, y1 = x0+width, y0 + height\n",
    "\n",
    "#         cp = [(0.5*(x0+x1), (y0-2.*pad)), # bottom\n",
    "#               (x1, y0),  # right lower-mid\n",
    "#               (x1+pad, (y0+y1)/2.),  # far right mid\n",
    "#               (x1, y1+pad),  # top right corner\n",
    "#               ((x0+x1)/2,y1),  # dip from top\n",
    "#               (x0, y1+pad),  # top left corner\n",
    "#               (x0-pad, (y0+y1)/2.),    # far left mid\n",
    "#               (x0, y0),  # left lower-mid\n",
    "#               ((x0+x1)/2., (y0-2.*pad)),  # return to bottom\n",
    "#               ((x0+x1)/2., (y0-2.*pad))]\n",
    "        \n",
    "        \n",
    "#         com = [Path.MOVETO,  # start\n",
    "#                Path.CURVE4, # curve to right mid\n",
    "#                Path.LINETO,  # line to far right mid\n",
    "#                Path.LINETO, # line to top right corner\n",
    "#                Path.CURVE3,  # line to dip\n",
    "#                Path.LINETO,  # line to top left corner\n",
    "#                Path.CURVE4,   # curve to far left mid\n",
    "#                Path.LINETO,   # line to left lower-mid\n",
    "#                Path.LINETO,\n",
    "#                Path.CLOSEPOLY]\n",
    "\n",
    "#         path = Path(cp, com)\n",
    "\n",
    "#         return path\n",
    "\n",
    "# BoxStyle._style_list[\"shield\"] = shield\n",
    "\n",
    "\n",
    "\n",
    "# def parse_road_name(thename):\n",
    "#     try:\n",
    "#         annotator = thisrow['FULLNAME'][re.search('\\d',thisrow['FULLNAME']).start():]\n",
    "#     except AttributeError:\n",
    "#         try: \n",
    "#             annotator = thisrow['FULLNAME'][re.search('Hwy',thisrow['FULLNAME']).end():]\n",
    "#         except AttributeError:\n",
    "#             annotator = thisrow['FULLNAME']\n",
    "#     return annotator\n",
    "\n",
    "\n",
    "# road_label_format = { \n",
    "#     'I': { 'labelsize': 16, 'thecolor': 'yellow', 'thebbox': dict(boxstyle=\"shield\", fc='blue', ec='orange') },     \n",
    "#     'U': { 'labelsize': 14, 'thecolor': 'black', 'thebbox': dict(boxstyle=\"shield\", fc='white', ec='black') },\n",
    "#     'S': { 'labelsize': 12, 'thecolor': 'black', 'thebbox': dict(boxstyle=\"square,pad=0.25\", fc='white', ec='black')},\n",
    "#     'C': { 'labelsize': 10, 'thecolor': 'black', 'thebbox': dict(boxstyle=\"sawtooth,pad=0.5\", fc='white') }, \n",
    "#     'M': { 'labelsize': 16, 'thecolor': 'black'},\n",
    "#     'O': { 'labelsize': 11, 'thecolor': 'yellow' }\n",
    "# }\n",
    "# print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = time.time()\n",
    "# tract_gdf['new_district'].to_csv(script_dir+'new-districts-1-to-7.csv')\n",
    "# e = time.time()\n",
    "# g += (e-s)\n",
    "# print('Classified and saved {0:,.0f} in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(tract_gdf), np.floor((e-s)/60), (e-s)%60))\n",
    "# print('\\n')\n",
    "# print('TOTAL TIME: {0:,.0f} minutes {1:,.0f} seconds!'.format( np.floor(g/60), g%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
